{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, inspect, sys\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import PIL\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from dataset import NormalizedDataset, IncompleteDataset, get_stats, get_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "white = 255\n",
    "black = 0\n",
    "\n",
    "def inverse_color(img):\n",
    "    return PIL.Image.eval(img, lambda val: 255 - val)\n",
    "\n",
    "def fix_background_color_bug(img):\n",
    "    colors = sorted(img.getcolors(), key=lambda pair: pair[0], reverse=True)\n",
    "    replace_color = colors[0][1]\n",
    "    remove_color = colors[2][1] if colors[2][1] < colors[1][1] else colors[1][1]\n",
    "\n",
    "    data = np.array(img)\n",
    "    data[data == remove_color] = replace_color\n",
    "    return PIL.Image.fromarray(data)\n",
    "\n",
    "def remove_background(img):\n",
    "    return PIL.Image.eval(img, lambda val: 0 if val < (256/2) else val)\n",
    "\n",
    "def to_binary(img):\n",
    "    return PIL.Image.eval(img, lambda val: 255 if val < (256/2) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_image = False\n",
    "\n",
    "## Rotation, Brightness and Resizing\n",
    "random_transforms = [\n",
    "    transforms.RandomRotation(360, fill=black),\n",
    "    transforms.RandomAffine(0, shear=15, scale=(0.8, 0.95), translate=(.03, .03)),\n",
    "]\n",
    "\n",
    "format_transforms = [\n",
    "    transforms.Resize((25, 25)),\n",
    "]\n",
    "\n",
    "if not binary_image:\n",
    "    random_transforms.append(transforms.ColorJitter(brightness=(0.9, 1), contrast=(0.7, 1)))\n",
    "else:\n",
    "    format_transforms.append(transforms.Lambda(to_binary))\n",
    "\n",
    "all_transforms = transforms.Compose(random_transforms + format_transforms)\n",
    "format_transforms = transforms.Compose(format_transforms)\n",
    "\n",
    "## Adding Grayscale + Inverse color to operators\n",
    "operators_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Lambda(inverse_color),\n",
    "    all_transforms,\n",
    "])\n",
    "\n",
    "video_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Lambda(inverse_color),\n",
    "    transforms.Lambda(remove_background),\n",
    "    format_transforms,\n",
    "])\n",
    "\n",
    "operators_tensor_transform = transforms.Compose([\n",
    "    operators_transform,\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "video_tensor_transform = transforms.Compose([\n",
    "    video_transform,\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABkAAAAZCAAAAADhgtq/AAABrklEQVR4nI2SzUsbYRDGn3nfdz8SY4TVgNjG1rYWAoWKiBeDipce2kuhR89S+r949KSlf0CvpS1UKEIPLRQl4iL4gaCxRYjaNGt0d7MzHrbqeutcZuDHzDw8M8B/hn5wXZrbZGr65+Z2WlKa7lfp4zEwOTsQ++92b3qoPHdPlxeaYy8HC2ReLR1fE3fuSbfKJYulgbylrR+nmR6vx6G+8SQu2jppBpxR0Dq3YXqrkiMJ1+vpagUA4eeTGHA9z2aOLceu3mizn732tIgWofOjg07v768rSTpNoqCoCUqEnJKnaPDO6ZoBMFQqvbCYlQYg5Aop7nNhgOHZxznLtQAwIKxIFAtggMpIwdKkARYSYeKLTvsohAFM3jFaSIhBzMx/6y1/uQEDxBd56TA0kSQCChvz9U4EGGBvbRiheJ4FSSCsnbu7SerBhpSFH032aIBBZIrPv7f/ueP7AE8IFEXicmz3z3y4cgeA3WVEmlv7AaC7K8gQ1xWOdhY+rbbAZ7+y19ZgE3yp1ZbfPI1X3mdJ+w+UvwE03o4m36IsWX1Yodo2gMND3PoQwCsgOMm+0SX3J6w1fCtkKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=25x25 at 0x1297EC2B0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../video_dataset/op\"\n",
    "op_video_dataset = datasets.ImageFolder(root=path, transform=video_tensor_transform)\n",
    "datasets.ImageFolder(root=path, transform=video_transform)[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABkAAAAZCAAAAADhgtq/AAAA8klEQVR4nGNkYcABmHBJkC/DxMeJKcPCwMDAauvyec+V72gyjCwMDAaZPAw/Lu2+9QddhjXLjIGBgeHzmd1P/qKaxiHOwMDAwMDroHfk0Mv/yDKMjFDtwr4mu09+QjKNJd6REcb9c2f3hR8IF4iH6HPDDflxdeftXzAZBnYtN01WuNyXk9tewGQYGLhNneWZYVL/z8z4AZdhYBCycBKHBdX7tmeQMGBgYGBgeLfjnKuZEITNxMSAHKL/XiybcAgSRB8+MyDpYWBg+Hv38Uk3DXaGv2c+MSDZAwVchi5Cl1d/xCLDwMjD9R7hH6yABukAKwAAD/I//zg8XmYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=25x25 at 0x12980C518>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Operators Dataset\n",
    "path = '../operators'\n",
    "operators_dataset = datasets.ImageFolder(root=path, transform=operators_tensor_transform)\n",
    "\n",
    "# Show exemple image\n",
    "datasets.ImageFolder(root=path, transform=operators_transform)[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing mean and std\n",
    "operators_mean, operators_std = get_stats(operators_dataset)\n",
    "\n",
    "# Normalizing\n",
    "operators_dataset = NormalizedDataset(operators_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset and loader with only the video images\n",
    "op_incomplete_dataset = IncompleteDataset(op_video_dataset, operators_dataset.classes, operators_mean, operators_std)\n",
    "test_loader = torch.utils.data.DataLoader(op_incomplete_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training set\n",
    "train_loader = torch.utils.data.DataLoader(operators_dataset, batch_size=100)\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.regularizers import L1L2\n",
    "# from keras.layers import Conv2D\n",
    "# Creating a Net class object, which consists of 2 convolutional layers, max-pool layers and fully-connected layers\n",
    "class Conv_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, nb_hidden=25):        \n",
    "        super(Conv_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)  # the first convolutional layer, which processes the input image\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)  # the second convolutional layer, which gets the max-pooled set\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)  # the second convolutional layer, which gets the max-pooled set\n",
    "\n",
    "        self.fc1 = nn.Linear(64, nb_hidden)  # the first fully-connected layer, which gets flattened max-pooled set\n",
    "        self.fc2 = nn.Linear(nb_hidden, 5)  # the second fully-connected layer that outputs the result\n",
    "\n",
    "    # Creating the forward pass\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The first two layers\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        \n",
    "#         x = nn.Dropout(p=0.5)(x)\n",
    "        \n",
    "        # The second two layers\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) \n",
    "        \n",
    "#         x = nn.Dropout(p=0.5)(x)\n",
    "        \n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), kernel_size=2)) \n",
    "\n",
    "        # Flattening the data set for fully-connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "    \n",
    "        # The first fully-connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = nn.Dropout(p=0.3)(x)\n",
    "        \n",
    "        # The second full-connected layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th epoch, test_acc: 0.000000, 2 != 0, 2 != 1, 2 != 3, 0 != 4, test acc: 0.000000\n",
      "102th epoch, test_acc: 0.625000, 0 != 1, 3 != 4, test acc: 0.351485\n",
      "203th epoch, test_acc: 0.750000, 0 != 1, 3 != 4, test acc: 0.576733\n",
      "304th epoch, test_acc: 0.750000, test acc: 0.844059\n",
      "405th epoch, test_acc: 0.875000, 3 != 4, test acc: 0.898515\n",
      "506th epoch, test_acc: 0.875000, test acc: 0.920792\n",
      "607th epoch, test_acc: 1.000000, test acc: 0.957921\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = Conv_Net()\n",
    "\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Defining the optimizer for GD\n",
    "lr = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr) \n",
    "\n",
    "# Defining the criterion to calculate loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "nb_epochs = 50000\n",
    "accs = []\n",
    "best_acc = 0\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    # Train the input dataset by dividing it into mini_batch_size small datasets\n",
    "    acc = 0\n",
    "    model.train()\n",
    "    for train_input, train_target in train_loader:\n",
    "\n",
    "        # Model computations\n",
    "        output = model(train_input)\n",
    "        loss = criterion(output, train_target) \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tmp_acc = 0\n",
    "        for tensor, target in zip(output, train_target):\n",
    "            _, index = tensor.max(0)\n",
    "            if index == target:\n",
    "                tmp_acc += 1\n",
    "                \n",
    "        tmp_acc /= len(output)\n",
    "        acc += tmp_acc \n",
    "        \n",
    "    acc /= len(train_loader)\n",
    "    \n",
    "    should_print = e%101 == 0\n",
    "    if should_print:\n",
    "        print('%dth epoch, test_acc: %f' % (e+1, acc), end=\"\")\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Train the input dataset by dividing it into mini_batch_size small datasets\n",
    "    acc = 0\n",
    "    model.eval()\n",
    "    for test_input, test_target in test_loader:\n",
    "        \n",
    "        output = model(test_input)\n",
    "        loss = criterion(output, test_target) \n",
    "        \n",
    "        tmp_acc = 0\n",
    "        for tensor, target in zip(output, test_target):\n",
    "            _, index = tensor.max(0)\n",
    "            if index == target:\n",
    "                tmp_acc += 1\n",
    "            elif should_print:\n",
    "                print(\", {} != {}\".format(index, target), end=\"\")\n",
    "                \n",
    "        tmp_acc /= len(output)\n",
    "        acc += tmp_acc \n",
    "                \n",
    "    acc /= len(test_loader)\n",
    "    accs.append(acc)\n",
    "    \n",
    "    if should_print:\n",
    "        current_acc = sum(accs) / len(accs)\n",
    "        if current_acc > best_acc:\n",
    "            best_acc = current_acc\n",
    "            torch.save(model.state_dict(), \"operator_model\")\n",
    "            \n",
    "        print(\", test acc: %f\" % current_acc)\n",
    "        accs = []\n",
    "    test_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv_Net()\n",
    "model.load_state_dict(torch.load(\"operator_model\"))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(op_incomplete_dataset, batch_size=100)\n",
    "acc = 0\n",
    "for test_input, test_target in test_loader:\n",
    "\n",
    "    output = model(test_input)\n",
    "    loss = criterion(output, test_target) \n",
    "\n",
    "    tmp_acc = 0\n",
    "    for tensor, target in zip(output, test_target):\n",
    "        _, index = tensor.max(0)\n",
    "        if index == target:\n",
    "            tmp_acc += 1\n",
    "        elif True:\n",
    "            print(\", {} != {}\".format(index, target), end=\"\")\n",
    "\n",
    "    tmp_acc /= len(output)\n",
    "    acc += tmp_acc \n",
    "\n",
    "acc /= len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
