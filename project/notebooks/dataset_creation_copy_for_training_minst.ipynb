{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import PIL\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "white = 255\n",
    "black = 0\n",
    "\n",
    "def inverse_color(img):\n",
    "    return PIL.Image.eval(img, lambda val: 255 - val)\n",
    "\n",
    "def fix_background_color_bug(img):\n",
    "    colors = sorted(img.getcolors(), key=lambda pair: pair[0], reverse=True)\n",
    "    replace_color = colors[0][1]\n",
    "    remove_color = colors[2][1] if colors[2][1] < colors[1][1] else colors[1][1]\n",
    "\n",
    "    data = np.array(img)\n",
    "    data[data == remove_color] = replace_color\n",
    "    return PIL.Image.fromarray(data)\n",
    "\n",
    "def remove_background(img):\n",
    "    return PIL.Image.eval(img, lambda val: 0 if val < (256/2) else val)\n",
    "\n",
    "def to_binary(img):\n",
    "    return PIL.Image.eval(img, lambda val: 255 if val < (256/2) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_image = True\n",
    "\n",
    "\n",
    "## Rotation, Brightness and Resizing\n",
    "random_transforms = [\n",
    "    transforms.RandomRotation(360, fill=black),\n",
    "    transforms.RandomAffine(0, shear=15, scale=(0.8, 0.95), translate=(.03, .03)),\n",
    "]\n",
    "\n",
    "format_transforms = [\n",
    "    transforms.Resize((40, 40)),\n",
    "]\n",
    "\n",
    "if not binary_image:\n",
    "    random_transforms.append(transforms.ColorJitter(brightness=(0.9, 1), contrast=(0.7, 1)))\n",
    "else:\n",
    "    format_transforms.append(transforms.Lambda(to_binary))\n",
    "\n",
    "all_transforms = transforms.Compose(random_transforms + format_transforms)\n",
    "format_transforms = transforms.Compose(format_transforms)\n",
    "\n",
    "## Adding Grayscale + Inverse color to operators\n",
    "operators_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Lambda(inverse_color),\n",
    "    # Randomly scale up and down\n",
    "#     transforms.RandomAffine(0, scale=(0.9, 1.1), fillcolor=white),\n",
    "    all_transforms,\n",
    "])\n",
    "\n",
    "video_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Lambda(inverse_color),\n",
    "    transforms.Lambda(remove_background),\n",
    "    format_transforms,\n",
    "])\n",
    "\n",
    "operators_tensor_transform = transforms.Compose([\n",
    "    operators_transform,\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "minst_tensor_transform = transforms.Compose([\n",
    "    all_transforms,\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "video_tensor_transform = transforms.Compose([\n",
    "    video_transform,\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAAAAACpleexAAAArklEQVR4nLWVwQrDMAxDpdD//2XtMELtVE5M2XRqzIsqbEMo9DSa3B/Aq6gTQIq/c2QFkswMPUgAzAQdSPM/GTACCTYZawoAtQFif15M5rAdwfFJMiSict1KaGZkFwQeI6x1daDb8cx9wQZXbniUWuDs8wG8xzHysdZYLpaas6awX8kRC8aYK3jUEZyWwxX7jnqmtQ0XoOk/r2wy5kY4UO6TKQ0XLig7bkbJn78KH5OLIFhvsY8JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=40x40 at 0x1217A65C0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_video_dataset = datasets.ImageFolder(root='video_dataset/op', transform=video_tensor_transform)\n",
    "minst_video_dataset = datasets.ImageFolder(root='video_dataset/minst', transform=video_tensor_transform)\n",
    "datasets.ImageFolder(root='video_dataset/op', transform=video_transform)[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAAAAACpleexAAAAq0lEQVR4nLWUSw6AIAxEO4T7X7kuIDraKVajLjDAo9MfwK32tSL3OwikICiYQSXR9St3mtKhFjjQDwqM9k6kACEnB+hyfykNwTHoRMbFV5URqT6WuinPxdFW5Oo+9tUmd0nB4ui9kjSq4LRYu4fdBpnniECrwG/yGAzuKm58C4WwLOFdNI/7kZtaehMtJvkPoDMrHoDbOu4Wp2c8ZtIeCX7qEk1cuBRcSX8Ebq0oI1bStIgtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=40x40 at 0x12722BDA0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Operators Dataset\n",
    "operators_dataset = datasets.ImageFolder(root='operators', transform=operators_tensor_transform)\n",
    "\n",
    "# Show exemple image\n",
    "datasets.ImageFolder(root='operators', transform=operators_transform)[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAAAAACpleexAAAAk0lEQVR4nNWUwQrAMAhDtez/f9kdWjaqiUpPW2GX8khsIlOT3hlN7t+g6qHiBbRETObHwdfPFzEIl1knVLSGrt7aMu4sHsvmVG+kxDyASzVcgxntoQtwkZ2lsPmwjcXxmBVdoxkqUGV/EepaUPBta6wIMoeKqHFu7fKhoM8xzKgQo83EK9Z1CbK9RYuLzxf/j8fgDdguHVZnMbS7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=40x40 at 0x12722B048>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minst Dataset\n",
    "minst_dataset = datasets.MNIST(\"\", transform=minst_tensor_transform, download=True)\n",
    "\n",
    "# Show example image\n",
    "datasets.MNIST(\"\", transform=all_transforms, download=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(dataset):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=50,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "\n",
    "    mean /= len(loader.dataset)\n",
    "    std /= len(loader.dataset)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7394]), tensor([0.4311]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operators_mean, operators_std = get_stats(operators_dataset)\n",
    "operators_mean, operators_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9004]), tensor([0.2934]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minst_mean, minst_std = get_stats(minst_dataset)\n",
    "minst_mean, minst_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionDataset(datasets.VisionDataset):\n",
    "    \"\"\"Custom Dataset for loading CelebA face images\"\"\"\n",
    "\n",
    "    def __init__(self, operators_dataset, minst_dataset, video_dataset=[]):\n",
    "        self.operators_dataset = operators_dataset\n",
    "        self.minst_dataset = minst_dataset\n",
    "        self.video_dataset = video_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.video_dataset):\n",
    "            tensor, class_index = self.video_dataset[index]   \n",
    "            tensor = transforms.Normalize(operators_mean, operators_std)(tensor)\n",
    "            \n",
    "            # Get actual index\n",
    "            class_str = self.video_dataset.classes[class_index]\n",
    "            new_index = self.classes.index(class_str)\n",
    "            \n",
    "            return tensor, new_index\n",
    "        elif index < len(self.video_dataset) + len(self.operators_dataset):\n",
    "            tensor, class_index = self.operators_dataset[index - len(self.video_dataset)]   \n",
    "            tensor = transforms.Normalize(operators_mean, operators_std)(tensor)\n",
    "            return tensor, class_index + len(self.minst_dataset.classes)\n",
    "        elif index < len(self.video_dataset) + len(self.operators_dataset) + len(self.minst_dataset):\n",
    "            tensor, class_index = self.minst_dataset[index - len(self.operators_dataset) - len(self.video_dataset)]\n",
    "            tensor = transforms.Normalize(minst_mean, minst_std)(tensor)\n",
    "            return tensor, class_index\n",
    "        else:\n",
    "            random_index = randint(0, len(self.operators_dataset)-1)\n",
    "            tensor, class_index = self.operators_dataset[random_index]\n",
    "            tensor = transforms.Normalize(operators_mean, operators_std)(tensor)\n",
    "            return tensor, class_index + len(self.minst_dataset.classes)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.minst_dataset) + len(self.operators_dataset)\n",
    "    \n",
    "    @property\n",
    "    def classes(self):\n",
    "        minst_classes = [s[:1] for s in self.minst_dataset.classes]\n",
    "        return minst_classes + self.operators_dataset.classes\n",
    "    \n",
    "    @property\n",
    "    def targets(self):\n",
    "        video_tar = self.video_dataset.targets if self.video_dataset else []\n",
    "        op_tar = [tar+len(self.minst_dataset.classes) for tar in self.operators_dataset.targets]\n",
    "        return  list(self.minst_dataset.targets.numpy()) + op_tar\n",
    "    \n",
    "def make_weights_for_balanced_classes(targets, nclasses):  \n",
    "    count = [0] * nclasses                                                      \n",
    "    for target in targets:                                                         \n",
    "        count[target] += 1                                                     \n",
    "    weight_per_class = [0.] * nclasses                                      \n",
    "    N = float(sum(count))                                                   \n",
    "    for i in range(nclasses):                                                   \n",
    "        weight_per_class[i] = N/float(count[i])                                 \n",
    "    weight = [0] * len(targets)                                              \n",
    "    for idx, target in enumerate(targets):                                          \n",
    "        weight[idx] = weight_per_class[target]                                  \n",
    "    return weight \n",
    "    \n",
    "def balanced_loader(dataset, batch_size=100, validation_split=0.2, shuffle_dataset=True):                                                                    \n",
    "    # For unbalanced dataset we create a weighted sampler                       \n",
    "    weights = make_weights_for_balanced_classes(dataset.targets, len(dataset.classes))                                                                \n",
    "    weights = torch.DoubleTensor(weights)                                       \n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))                     \n",
    "\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FusionDataset(operators_dataset, minst_dataset, [])\n",
    "# mean, std = get_stats(dataset)\n",
    "# mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'divide',\n",
       " 'equal',\n",
       " 'minus',\n",
       " 'multiply',\n",
       " 'plus']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedDataset:\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.mean, self.std = get_stats(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tensor, class_ = self.dataset[index]\n",
    "        normalized = transforms.Normalize(mean=self.mean, std=self.std)(tensor)\n",
    "        return normalized, class_\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "class IncompleteDataset:\n",
    "    \n",
    "    def __init__(self, dataset, all_classes):\n",
    "        self.dataset = dataset\n",
    "        self.all_classes = all_classes\n",
    "        self.mean, self.std = get_stats(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tensor, class_ = self.dataset[index]\n",
    "        class_str = self.dataset.classes[class_]\n",
    "        new_index = self.all_classes.index(class_str)\n",
    "        normalized = transforms.Normalize(mean=self.mean, std=self.std)(tensor)\n",
    "        return normalized, new_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "op_incomplete_dataset = IncompleteDataset(op_video_dataset, operators_dataset.classes)\n",
    "\n",
    "minst_classes = [word[:1] for word in minst_dataset.classes]\n",
    "minst_incomplete_dataset = IncompleteDataset(minst_video_dataset, minst_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(NormalizedDataset(minst_dataset), batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(minst_incomplete_dataset, batch_size=100)\n",
    "len(train_loader.dataset), len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.regularizers import L1L2\n",
    "# from keras.layers import Conv2D\n",
    "# Creating a Net class object, which consists of 2 convolutional layers, max-pool layers and fully-connected layers\n",
    "class Conv_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, nb_hidden=50):        \n",
    "        super(Conv_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)  # the first convolutional layer, which processes the input image\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)  # the second convolutional layer, which gets the max-pooled set\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)  # the second convolutional layer, which gets the max-pooled set\n",
    "\n",
    "        self.fc1 = nn.Linear(576, nb_hidden)  # the first fully-connected layer, which gets flattened max-pooled set\n",
    "        self.fc2 = nn.Linear(nb_hidden, 15)  # the second fully-connected layer that outputs the result\n",
    "\n",
    "    # Creating the forward pass\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The first two layers\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        \n",
    "#         x = nn.Dropout(p=0.5)(x)\n",
    "        \n",
    "        # The second two layers\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2)) \n",
    "        \n",
    "#         x = nn.Dropout(p=0.5)(x)\n",
    "        \n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), kernel_size=2)) \n",
    "\n",
    "        # Flattening the data set for fully-connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "    \n",
    "        # The first fully-connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = nn.Dropout(p=0.3)(x)\n",
    "        \n",
    "        # The second full-connected layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th epoch, test_acc: 0.438733, 3 != 7, 3 != 7, test acc: 0.666667\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = Conv_Net()\n",
    "\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Defining the optimizer for GD\n",
    "lr = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr) \n",
    "\n",
    "# Defining the criterion to calculate loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "nb_epochs = 30000\n",
    "\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    # Train the input dataset by dividing it into mini_batch_size small datasets\n",
    "    acc = 0\n",
    "    for train_input, train_target in train_loader:\n",
    "\n",
    "        # Model computations\n",
    "        output = model(train_input)\n",
    "        loss = criterion(output, train_target) \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tmp_acc = 0\n",
    "        for tensor, target in zip(output, train_target):\n",
    "            _, index = tensor.max(0)\n",
    "            if index == target:\n",
    "                tmp_acc += 1\n",
    "                \n",
    "        tmp_acc /= len(output)\n",
    "        acc += tmp_acc \n",
    "        \n",
    "    acc /= len(train_loader)\n",
    "    \n",
    "    should_print = e%1 == 0\n",
    "    if should_print:\n",
    "        print('%dth epoch, test_acc: %f' % (e+1, acc), end=\"\")\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Train the input dataset by dividing it into mini_batch_size small datasets\n",
    "    acc = 0\n",
    "    for test_input, test_target in test_loader:\n",
    "        \n",
    "        output = model(test_input)\n",
    "        loss = criterion(output, test_target) \n",
    "        \n",
    "        tmp_acc = 0\n",
    "        for tensor, target in zip(output, test_target):\n",
    "            _, index = tensor.max(0)\n",
    "            if index == target:\n",
    "                tmp_acc += 1\n",
    "            elif should_print:\n",
    "                print(\", {} != {}\".format(index, target), end=\"\")\n",
    "                \n",
    "        tmp_acc /= len(output)\n",
    "        acc += tmp_acc \n",
    "                \n",
    "    acc /= len(test_loader)\n",
    "    if should_print:\n",
    "        print(\", test acc: %f\" % (acc))\n",
    "    test_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats(incomplete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(incomplete_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for test_input, test_target in loader:\n",
    "\n",
    "    output = model(test_input)\n",
    "    loss = criterion(output, test_target) \n",
    "\n",
    "    tmp_acc = 0\n",
    "    for tensor, target in zip(output, test_target):\n",
    "#         print(tensor, target)\n",
    "        _, index = tensor.max(0)\n",
    "#         print(tensor, index)\n",
    "        if index == target:\n",
    "            tmp_acc += 1\n",
    "\n",
    "    tmp_acc /= len(output)\n",
    "    acc += tmp_acc \n",
    "\n",
    "acc /= len(test_loader)\n",
    "print(\"test loss: %f, acc: %f\" % (loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-cfa78b3e57b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mincomplete_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-157-6998a67859a7>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mclass_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mclass_index\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "incomplete_dataset[15][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_dataset[i][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
